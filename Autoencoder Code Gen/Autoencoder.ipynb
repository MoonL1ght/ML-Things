{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16678039\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Подготовка данных</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 21941/21941 [00:41<00:00, 526.75it/s]\n"
     ]
    }
   ],
   "source": [
    "SOURCE_PATH = './data/sources/'\n",
    "\n",
    "'''\n",
    "sources_ws - split by white spaces and \\n\n",
    "sources_ch - split by chars\n",
    "'''\n",
    "sources_ws = []\n",
    "sources_ch = []\n",
    "for file_name in tqdm(listdir(SOURCE_PATH)):\n",
    "    with open(path.join(SOURCE_PATH, file_name), 'r', encoding=\"ansi\") as file:\n",
    "        data=file.read()\n",
    "        sources_ws.append(data.split())\n",
    "        sources_ch.append([ord(char) for char in list(data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21941\n",
      "21941\n"
     ]
    }
   ],
   "source": [
    "print(len(sources_ws))\n",
    "print(len(sources_ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная длина разбинения по пробелам:  289\n",
      "Минимальная длина разбинения по пробелам:  1\n",
      "Максимальная длина разбинения по символам:  1023\n",
      "Минимальная длина разбинения по символам:  7\n"
     ]
    }
   ],
   "source": [
    "lengths_ws = [len(data) for data in sources_ws]\n",
    "lengths_ch = [len(data) for data in sources_ch]\n",
    "max_l_ws = max(lengths_ws)\n",
    "min_l_ws = min(lengths_ws)\n",
    "max_l_ch = max(lengths_ch)\n",
    "min_l_ch = min(lengths_ch)\n",
    "print('Максимальная длина разбинения по пробелам: ', max_l_ws)\n",
    "print('Минимальная длина разбинения по пробелам: ', min_l_ws)\n",
    "print('Максимальная длина разбинения по символам: ', max_l_ch)\n",
    "print('Минимальная длина разбинения по символам: ', min_l_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_SYMBOL = ord(' ')\n",
    "flat_sources_ch = [ch for source in sources_ch for ch in source]\n",
    "flat_sources_ch.append(PAD_SYMBOL)\n",
    "unique_ch = set(flat_sources_ch)\n",
    "ch2index = {ch: i for (i, ch) in enumerate(unique_ch)}\n",
    "index2ch = {i: ch for (i, ch) in enumerate(unique_ch)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [ch for source in sources_ch for ch in source]\n",
    "ch_encoder = OneHotEncoder()\n",
    "ch_one_hot = ch_encoder.fit_transform(np.reshape(a, (-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_one_hot_sources = []\n",
    "end_index = 0\n",
    "for l in lengths_ch:\n",
    "    ch_one_hot_sources.append(ch_one_hot[end_index:end_index+l])\n",
    "    end_index += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, batch_size):\n",
    "    x_batch = np.array([seq.toarray() for seq in x[:batch_size]])\n",
    "    seq_lengths = [seq.shape[0] for seq in x_batch]\n",
    "    return x_batch, seq_lengths\n",
    "\n",
    "def next_batch(x, size):\n",
    "    idx = np.arange(0 , len(x))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:size]\n",
    "    x_batch = np.array([x[i].toarray() for i in idx])\n",
    "    seq_lengths = [seq.shape[0] for seq in x_batch]\n",
    "    return x_batch, seq_lengths\n",
    "\n",
    "def add_padding(batch, pad_size, padding):\n",
    "    result = np.empty((0, pad_size, batch[0].shape[1]), np.float32)\n",
    "    for seq in batch:\n",
    "        if len(seq) < pad_size:\n",
    "            seq = np.concatenate([seq, [padding for _ in range(pad_size - len(seq))]])\n",
    "        result = np.append(result, [seq], axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "ENCODER_LAYERS = [1024, 512]\n",
    "DECODER_LAYERS = [1024]\n",
    "VEC_DIM = len(unique_ch)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, None, VEC_DIM])\n",
    "Y = tf.placeholder(tf.float32, [None, None, VEC_DIM])\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "with tf.variable_scope('encoder'):\n",
    "    encoder_layers = [tf.contrib.rnn.BasicRNNCell(size) for size in ENCODER_LAYERS]\n",
    "    encoder_layer_cell = tf.contrib.rnn.MultiRNNCell(encoder_layers)\n",
    "    _, encoder_state = tf.nn.dynamic_rnn(cell=encoder_layer_cell,\n",
    "                                         inputs=X,\n",
    "                                         dtype=tf.float32,\n",
    "                                         sequence_length = seq_length)\n",
    "\n",
    "\n",
    "with tf.variable_scope('decoder'):\n",
    "    decoder_input_pad = tf.zeros((tf.shape(X)[0], tf.shape(X)[1]-1, ENCODER_LAYERS[-1]))\n",
    "    decoder_input = tf.concat([tf.reshape(encoder_state[-1], (-1, 1, ENCODER_LAYERS[-1])), decoder_input_pad], axis=1)\n",
    "    decoder_layers = [tf.contrib.rnn.BasicRNNCell(size) for size in DECODER_LAYERS + [VEC_DIM]]\n",
    "    decoder_layer_cell = tf.contrib.rnn.MultiRNNCell(decoder_layers)\n",
    "    decoder_out, decoder_state = tf.nn.dynamic_rnn(cell=decoder_layer_cell,\n",
    "                                                   inputs=decoder_input,\n",
    "                                                   dtype=tf.float32,\n",
    "                                                   sequence_length = seq_length)\n",
    "    logit = tf.nn.softmax(decoder_out, axis=2)\n",
    "    \n",
    "    target = tf.argmax(Y, axis=2)\n",
    "#     t = tf.reduce_sum(logit, axis=2)\n",
    "    target_weights = tf.cast(tf.sequence_mask(seq_length, tf.reduce_max(seq_length)), tf.float32)\n",
    "#     rmse_loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(targets, outputs))))\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logit, target, target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[515]\n",
      "(1, 515, 113)\n",
      "---=-=\n",
      "(1, 515, 113)\n",
      "(1, 10, 113)\n",
      "---\n",
      "(1, 512)\n",
      "(1, 10, 512)\n",
      "(1, 10, 113)\n",
      "[[[0.00823788 0.01020501 0.00885439 ... 0.01071051 0.00840354 0.01011684]\n",
      "  [0.00990466 0.01107283 0.00885214 ... 0.00907372 0.00723025 0.00864497]\n",
      "  [0.00848261 0.00834738 0.00850012 ... 0.00746135 0.00885945 0.00923185]\n",
      "  ...\n",
      "  [0.00851918 0.00858712 0.00948008 ... 0.00922577 0.00895549 0.00855347]\n",
      "  [0.00863538 0.00919989 0.00831656 ... 0.00937999 0.00932554 0.00901209]\n",
      "  [0.00853429 0.00920118 0.00876028 ... 0.00854011 0.00897201 0.00857319]]]\n",
      "[[20 15  5 59 74 87 78 75 94 31]]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "4.727465\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "x_batch, batch_seq_len = next_batch(ch_one_hot_sources, BATCH_SIZE)\n",
    "print(batch_seq_len)\n",
    "print(x_batch.shape)\n",
    "# print(x_batch[:,:10,:].shape)\n",
    "# print(x_batch[1].shape)\n",
    "print('---=-=')\n",
    "x_batch = add_padding(x_batch, max(batch_seq_len), np.eye(VEC_DIM)[PAD_SYMBOL])\n",
    "print(x_batch.shape)\n",
    "x_batch = x_batch[:,:10,:]\n",
    "batch_seq_len = [10]\n",
    "print(x_batch.shape)\n",
    "# train_x = np.array(ch_one_hot_sources[:2])\n",
    "# dim = train_x[0].toarray()\n",
    "# print(dim)\n",
    "# print(train_x)\n",
    "print('---')\n",
    "print(sess.run(encoder_state, feed_dict={X: x_batch, seq_length: batch_seq_len})[-1].shape)\n",
    "# print(sess.run(decoder_input_pad, feed_dict={X: x_batch, seq_length: batch_seq_len}))\n",
    "print(sess.run(decoder_input, feed_dict={X: x_batch, seq_length: batch_seq_len}).shape)\n",
    "print(sess.run(decoder_out, feed_dict={X: x_batch, seq_length: batch_seq_len}).shape)\n",
    "print(sess.run(logit, feed_dict={X: x_batch, seq_length: batch_seq_len}))\n",
    "print(sess.run(target, feed_dict={Y: x_batch, seq_length: batch_seq_len}))\n",
    "print(sess.run(target_weights, feed_dict={Y: x_batch, seq_length: batch_seq_len}))\n",
    "print(sess.run(loss, feed_dict={X: x_batch, Y: x_batch, seq_length: batch_seq_len}))\n",
    "\n",
    "# print(sess.run(t, feed_dict={X: x_batch, seq_length: batch_seq_len}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  21721\n",
      "Test set size:  220\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.99\n",
    "X_train = ch_one_hot_sources[:int(train_size*len(ch_one_hot_sources))]\n",
    "X_test = np.array([seq.toarray() for seq in ch_one_hot_sources[int(train_size*len(ch_one_hot_sources)):]])\n",
    "print('Train set size: ', len(X_train))\n",
    "print('Test set size: ', len(X_test))\n",
    "test_seq_length = [seq.shape[0] for seq in X_test]\n",
    "max_test_seq_length = max(test_seq_length)\n",
    "X_test = add_padding(X_test, max_test_seq_length, np.eye(VEC_DIM)[PAD_SYMBOL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches:  2172\n",
      "(220, 1021, 113)\n",
      "Epoch 0, test loss: 4.727473\n",
      "4.7273808\n",
      "4.727367\n",
      "4.727393\n",
      "4.727385\n",
      "4.727394\n",
      "4.727403\n",
      "4.7273717\n",
      "4.727387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-380e5f6f4498>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m         }\n\u001b[0;32m     32\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 10\n",
    "EPOCHES = 1\n",
    "N_BATHCES = int(len(X_train)/BATCH_SIZE)\n",
    "print('Batches: ', N_BATHCES)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(EPOCHES):\n",
    "    print(X_test.shape)\n",
    "    test_feed_dict = {\n",
    "        X: X_test,\n",
    "        Y: X_test,\n",
    "        seq_length: test_seq_length\n",
    "    }\n",
    "    test_loss = sess.run(loss, feed_dict=test_feed_dict)\n",
    "    print('Epoch %u, test loss: %f' %(epoch, test_loss))\n",
    "    for i in range(N_BATHCES):\n",
    "        x_batch, batch_seq_len = next_batch(X_train, BATCH_SIZE)\n",
    "        x_batch = add_padding(x_batch, max(batch_seq_len), np.eye(VEC_DIM)[PAD_SYMBOL])\n",
    "        feed_dict = {\n",
    "            X: x_batch,\n",
    "            Y: x_batch,\n",
    "            seq_length: batch_seq_len\n",
    "        }\n",
    "        sess.run(loss, feed_dict=feed_dict)\n",
    "        train_loss = sess.run(loss, feed_dict=feed_dict)\n",
    "        print(train_loss)\n",
    "\n",
    "max_test_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
