{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avs_yPYOGoed"
   },
   "source": [
    "<center><h2>Подготовка работы в Colab</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "bLIi8mowEdTL",
    "outputId": "b67627f3-77e7-4ec8-bcd7-4dc7920a7e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.26.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-1e49a4b3-b6ef-4403-b71e-0f430442a28b\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-1e49a4b3-b6ef-4403-b71e-0f430442a28b\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sources.zip to sources.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8488758"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('sources.zip','wb').write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EO_1n4YGHpO"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "newpath = r'./data/sources' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "zip_ref = zipfile.ZipFile('sources.zip', 'r')\n",
    "zip_ref.extractall('./data/sources/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x4Q9AmXlGRiA",
    "outputId": "376c036f-36c9-4125-8125-49cdf53c345a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  sample_data  sources.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWGEg8JqEFCu"
   },
   "source": [
    "<h2><center> Подготовка данных</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFSduo4jEFCf"
   },
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_RIm1EIgEFCw",
    "outputId": "068612cd-b976-4492-aa3d-5e7f87311360"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21941/21941 [00:02<00:00, 9042.81it/s]\n"
     ]
    }
   ],
   "source": [
    "SOURCE_PATH = './data/sources/'\n",
    "\n",
    "'''\n",
    "sources_ch - split by chars\n",
    "'''\n",
    "sources_ch = []\n",
    "for file_name in tqdm(listdir(SOURCE_PATH)):\n",
    "    with open(path.join(SOURCE_PATH, file_name), 'r', encoding=\"cp1252\") as file:\n",
    "        data=file.read()\n",
    "        sources_ch.append([char for char in list(data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EMEkqpm9k9g"
   },
   "source": [
    "<h2><center> Обработка данных</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "D-yQmQ66UrHF",
    "outputId": "00ce78be-b7d3-4ca7-b722-8f7ed2584248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная длина разбинения по символам:  1023\n",
      "Минимальная длина разбинения по символам:  7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lengths_ch = [len(data) for data in sources_ch]\n",
    "\n",
    "max_l_ch = max(lengths_ch)\n",
    "min_l_ch = min(lengths_ch)\n",
    "\n",
    "print('Максимальная длина разбинения по символам: ', max_l_ch)\n",
    "print('Минимальная длина разбинения по символам: ', min_l_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XQa9e4sEFDD"
   },
   "outputs": [],
   "source": [
    "PAD_SYMBOL = ' '\n",
    "flat_sources_ch = [ch for source in sources_ch for ch in source]\n",
    "flat_sources_ch.append(PAD_SYMBOL)\n",
    "unique_ch = set(flat_sources_ch)\n",
    "ch2index = {ch: i for (i, ch) in enumerate(unique_ch)}\n",
    "index2ch = {i: ch for (i, ch) in enumerate(unique_ch)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEoGSvOQEFDG"
   },
   "outputs": [],
   "source": [
    "a = [ch2index[ch] for source in sources_ch for ch in source]\n",
    "ch_encoder = OneHotEncoder()\n",
    "ch_one_hot = ch_encoder.fit_transform(np.reshape(a, (-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ENIZAiiEFDJ"
   },
   "outputs": [],
   "source": [
    "ch_one_hot_sources = []\n",
    "end_index = 0\n",
    "for l in lengths_ch:\n",
    "    ch_one_hot_sources.append(ch_one_hot[end_index:end_index+l])\n",
    "    end_index += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_YMCCGoEFDM"
   },
   "outputs": [],
   "source": [
    "def get_batch(x, batch_size):\n",
    "    x_batch = np.array([seq.toarray() for seq in x[:batch_size]])\n",
    "    seq_lengths = [seq.shape[0] for seq in x_batch]\n",
    "    return x_batch, seq_lengths\n",
    "\n",
    "def next_batch(x, size):\n",
    "    idx = np.arange(0 , len(x))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:size]\n",
    "    x_batch = np.array([x[i].toarray() for i in idx])\n",
    "    seq_lengths = [seq.shape[0] for seq in x_batch]\n",
    "    return x_batch, seq_lengths\n",
    "\n",
    "def add_padding(batch, pad_size):\n",
    "    for i in range(len(batch)):\n",
    "        batch[i] = np.pad(batch[i], [(0, pad_size - len(batch[i])), (0,0)],\n",
    "                          mode='constant', constant_values=0)\n",
    "    return np.reshape(np.vstack(batch), (-1, batch[0].shape[0], batch[0].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JT_QhwrZ9oYL"
   },
   "source": [
    "<h2><center>Построение сети RNN VAE</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0xprFtOPx_X"
   },
   "source": [
    "Архитектура сети: RNN(seq2vec) -> (FC + FC) -> RNN(vec2seq) \n",
    "Генеративный Автоэнкодер на основе рекурентной сети. Для более хорошего обучения необходимо увеличивать размер сети. Подбирать различные параметры, можно несколько улучшить саму архитекуру сети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRFJISfPEFDP"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "ENCODER_LAYERS = [100]\n",
    "DECODER_LAYERS = [100]\n",
    "N_LATENT = 100\n",
    "VEC_DIM = len(unique_ch)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, None, VEC_DIM])\n",
    "Y = tf.placeholder(tf.float32, [None, None, VEC_DIM])\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "with tf.variable_scope('encoder'):\n",
    "    encoder_layers = [tf.nn.rnn_cell.GRUCell(size) for size in ENCODER_LAYERS]\n",
    "    encoder_layer_cell = tf.contrib.rnn.MultiRNNCell(encoder_layers)\n",
    "    encoder_out, encoder_state = tf.nn.dynamic_rnn(cell=encoder_layer_cell,\n",
    "                                         inputs=X,\n",
    "                                         dtype=tf.float32,\n",
    "                                         sequence_length = seq_length)\n",
    "\n",
    "with tf.variable_scope('latent_layer'):\n",
    "    stacked_encoder_out = tf.reshape(encoder_out, (-1, ENCODER_LAYERS[-1]))\n",
    "    mean = tf.contrib.layers.fully_connected(stacked_encoder_out, N_LATENT, activation_fn=None)\n",
    "    std_dev = 0.5 * tf.contrib.layers.fully_connected(stacked_encoder_out, N_LATENT, activation_fn=None)\n",
    "    epsilon = tf.random_normal((tf.shape(stacked_encoder_out)[0], N_LATENT))\n",
    "    z = mean + tf.multiply(epsilon, tf.exp(std_dev))\n",
    "    z_input = tf.reshape(z, [-1, tf.shape(encoder_out)[1], N_LATENT])\n",
    "    \n",
    "with tf.variable_scope('decoder'):\n",
    "    decoder_layers = [tf.nn.rnn_cell.GRUCell(size) for size in DECODER_LAYERS]\n",
    "    decoder_layer_cell = tf.contrib.rnn.MultiRNNCell(decoder_layers)\n",
    "    decoder_out, decoder_state = tf.nn.dynamic_rnn(cell=decoder_layer_cell,\n",
    "                                                   inputs=z_input,\n",
    "                                                   dtype=tf.float32,\n",
    "                                                   sequence_length = seq_length)\n",
    "\n",
    "with tf.variable_scope('fc'):\n",
    "    stacked_rnn_outputs = tf.reshape(decoder_out, (-1, DECODER_LAYERS[-1]))\n",
    "    fc= tf.contrib.layers.fully_connected(stacked_rnn_outputs, VEC_DIM, activation_fn=None)\n",
    "    logit = tf.reshape(fc, [-1, tf.shape(decoder_out)[1], VEC_DIM])\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    target = tf.argmax(Y, axis=2)\n",
    "    target_weights = tf.cast(tf.sequence_mask(seq_length, tf.reduce_max(seq_length)), tf.float32)\n",
    "    target_loss = tf.contrib.seq2seq.sequence_loss(logit, target, target_weights)\n",
    "    latent_loss = - 0.5 * tf.reduce_mean(1 + 2.0 * std_dev - tf.square(mean) - tf.exp(2.0 * std_dev), axis=-1)\n",
    "    loss = tf.reduce_mean(target_loss + latent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "YDYTSUCCEFDU",
    "outputId": "045660cb-8c5d-46b9-9a47-d7898db09de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[452, 127]\n",
      "(2,)\n",
      "---=-=\n",
      "(2, 452, 113)\n",
      "(2, 10, 113)\n",
      "---\n",
      "(2, 10, 100)\n",
      "(2, 10, 100)\n",
      "(2, 10, 100)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "x_batch, batch_seq_len = next_batch(ch_one_hot_sources, BATCH_SIZE)\n",
    "print(batch_seq_len)\n",
    "print(x_batch.shape)\n",
    "# print(x_batch[:,:10,:].shape)\n",
    "# print(x_batch[1].shape)\n",
    "print('---=-=')\n",
    "x_batch = add_padding(x_batch, max(batch_seq_len))\n",
    "print(x_batch.shape)\n",
    "x_batch = x_batch[:,:10,:]\n",
    "batch_seq_len = [10, 10]\n",
    "print(x_batch.shape)\n",
    "# train_x = np.array(ch_one_hot_sources[:2])\n",
    "# dim = train_x[0].toarray()\n",
    "# print(dim)\n",
    "# print(train_x)\n",
    "print('---')\n",
    "# print(sess.run(stacked_rnn_outputs, feed_dict={X: x_batch, seq_length: batch_seq_len})[-1].shape)\n",
    "# print(sess.run(mean, feed_dict={X: x_batch, seq_length: batch_seq_len}).shape)\n",
    "# print(sess.run(z, feed_dict={X: x_batch, seq_length: batch_seq_len}).shape)\n",
    "# print(sess.run(decoder_input, feed_dict={X: x_batch, seq_length: batch_seq_len}))\n",
    "print(sess.run(decoder_input, feed_dict={X: x_batch, seq_length: batch_seq_len}).shape)\n",
    "print(sess.run(encoder_out, feed_dict={X: x_batch, seq_length: batch_seq_len}).shape)\n",
    "print(sess.run(z_input, feed_dict={X: x_batch, seq_length: batch_seq_len}).shape)\n",
    "# print(sess.run(logit, feed_dict={decoder_input: [[[0,0,1],[1,0,0],[0,0,0]]], seq_length: [3]}).shape)\n",
    "# print(sess.run(logit, feed_dict={X: x_batch, seq_length: batch_seq_len}).shape)\n",
    "# print(sess.run(target, feed_dict={Y: x_batch, seq_length: batch_seq_len}))\n",
    "# print(sess.run(target_weights, feed_dict={Y: x_batch, seq_length: batch_seq_len}))\n",
    "# print(sess.run(target_loss, feed_dict={X: x_batch, Y: x_batch, seq_length: batch_seq_len}))\n",
    "# print(sess.run(latent_loss, feed_dict={X: x_batch, Y: x_batch, seq_length: batch_seq_len}))\n",
    "# print(sess.run(loss, feed_dict={X: x_batch, Y: x_batch, seq_length: batch_seq_len}))\n",
    "\n",
    "# print(sess.run(t, feed_dict={X: x_batch, seq_length: batch_seq_len}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyNPP-Vz9u0y"
   },
   "source": [
    "<h2><center> Обучение сети</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "VlZa_23kEFDc",
    "outputId": "5a6e21bf-59f0-4889-828e-05d47751223d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  21721\n",
      "Test set size:  220\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.99\n",
    "X_train = ch_one_hot_sources[:int(train_size*len(ch_one_hot_sources))]\n",
    "X_test = np.array([seq.toarray() for seq in ch_one_hot_sources[int(train_size*len(ch_one_hot_sources)):]])\n",
    "print('Train set size: ', len(X_train))\n",
    "print('Test set size: ', len(X_test))\n",
    "test_seq_length = [seq.shape[0] for seq in X_test]\n",
    "max_test_seq_length = max(test_seq_length)\n",
    "X_test = add_padding(X_test, max_test_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "Psxc4s0xEFDh",
    "outputId": "ac1bde59-1f0b-4c4e-a2c9-352d34bb3c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches:  2172\n",
      "(220, 1019, 113)\n",
      "Epoch 0, test loss: 4.760016\n",
      "Epoch 0, iteration 0 / 100 train loss: 4.527296\n",
      "Epoch 0, iteration 10 / 100 train loss: 3.535905\n",
      "Epoch 0, iteration 20 / 100 train loss: 3.598868\n",
      "Epoch 0, iteration 30 / 100 train loss: 2.627381\n",
      "Epoch 0, iteration 40 / 100 train loss: 1.274490\n",
      "Epoch 0, iteration 50 / 100 train loss: 0.581737\n",
      "Epoch 0, iteration 60 / 100 train loss: 0.342175\n",
      "Epoch 0, iteration 70 / 100 train loss: 0.207530\n",
      "Epoch 0, iteration 80 / 100 train loss: 0.178387\n",
      "Epoch 0, iteration 90 / 100 train loss: 0.131359\n",
      "(220, 1019, 113)\n",
      "Epoch 1, test loss: 0.115837\n",
      "Epoch 1, iteration 0 / 100 train loss: 0.142727\n",
      "Epoch 1, iteration 10 / 100 train loss: 0.114833\n",
      "Epoch 1, iteration 20 / 100 train loss: 0.122539\n",
      "Epoch 1, iteration 30 / 100 train loss: 0.121513\n",
      "Epoch 1, iteration 40 / 100 train loss: 0.102843\n",
      "Epoch 1, iteration 50 / 100 train loss: 0.111844\n",
      "Epoch 1, iteration 60 / 100 train loss: 0.133448\n",
      "Epoch 1, iteration 70 / 100 train loss: 0.106498\n",
      "Epoch 1, iteration 80 / 100 train loss: 0.098945\n",
      "Epoch 1, iteration 90 / 100 train loss: 0.087191\n",
      "(220, 1019, 113)\n",
      "Epoch 2, test loss: 0.080440\n",
      "Epoch 2, iteration 0 / 100 train loss: 0.108827\n",
      "Epoch 2, iteration 10 / 100 train loss: 0.088304\n",
      "Epoch 2, iteration 20 / 100 train loss: 0.092034\n",
      "Epoch 2, iteration 30 / 100 train loss: 0.090534\n",
      "Epoch 2, iteration 40 / 100 train loss: 0.080837\n",
      "Epoch 2, iteration 50 / 100 train loss: 0.099722\n",
      "Epoch 2, iteration 60 / 100 train loss: 0.079930\n",
      "Epoch 2, iteration 70 / 100 train loss: 0.080103\n",
      "Epoch 2, iteration 80 / 100 train loss: 0.077231\n",
      "Epoch 2, iteration 90 / 100 train loss: 0.079397\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 10\n",
    "EPOCHES = 3\n",
    "N_BATHCES = int(len(X_train)/BATCH_SIZE)\n",
    "ROUNDS = 100 # для ускорения обучения количество итераций уменьшено, сделано в целях тестирования\n",
    "print('Batches: ', N_BATHCES)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(EPOCHES):\n",
    "    print(X_test.shape)\n",
    "    test_feed_dict = {\n",
    "        X: X_test,\n",
    "        Y: X_test,\n",
    "        seq_length: test_seq_length\n",
    "    }\n",
    "    test_loss = sess.run(loss, feed_dict=test_feed_dict)\n",
    "    print('Epoch %u, test loss: %f' %(epoch, test_loss))\n",
    "    for i in range(ROUNDS):\n",
    "        x_batch, batch_seq_len = next_batch(X_train, BATCH_SIZE)\n",
    "        x_batch = add_padding(x_batch, max(batch_seq_len))\n",
    "#         print(x_batch.shape)\n",
    "        feed_dict = {\n",
    "            X: x_batch,\n",
    "            Y: x_batch,\n",
    "            seq_length: batch_seq_len\n",
    "        }\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if i % 10 == 0:\n",
    "            train_loss = sess.run(loss, feed_dict=feed_dict)\n",
    "            print('Epoch %u, iteration %u / %u train loss: %f' %(epoch, i, ROUNDS, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GD1ayNSPZTl"
   },
   "source": [
    "Проверка работы сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-WypJTSwRM"
   },
   "source": [
    " Для уменьшения времени обучения было сильно занижено количество раундов обучения, уменьшена архитектура сети. Но и на данном этапе видно что loss уменьшается, сеть обучается корректно. Для более лучших результатов необходимо дальньйшее подстраввание параметров/архитекутры сети. Использование embeddings, токенизация исходного кода (тренировачные данные), все это необходимо сделать для улучшения работы сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PaDMqaf-zaxv"
   },
   "source": [
    "работа автоэнкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "uKNdNVKkEFDn",
    "outputId": "02d90bfe-e5ed-4db2-b771-5aeaf5192a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin source chars data:\n",
      "/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n",
      "/* { dg-options \"-O2 -ftree-vectorize --save-temps\" } */\n",
      "\n",
      "#define TYPE double\n",
      "#include \"clastb_6.c\"\n",
      "\n",
      "/* { dg-final { scan-assembler {\\tclastb\\td[0-9]+, p[0-7], d[0-9]+, z[0-9]+\\.d} } } */\n",
      "\n",
      "Generated data:\n",
      "/* { dg-do assemble { target aarch64_lsm_sve_ok } } */\n",
      "/* { dg-options \"-O2 -ftree-vectorize --save-tempg\" } */\n",
      "\n",
      "#define TYPE double\n",
      "#include \"clastb_6.c\"\n",
      "\n",
      "/* { dg-final { scan-assembler {\\tclastb\\td[0-9]+, p[0-7], d[0-9]+, z[0-9]+\\.d} } } */\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Improvement to test: check encoding, maybe don't use ord and use char directly\n",
    "x_batch, batch_seq_len = next_batch(ch_one_hot_sources, 1)\n",
    "x_batch = add_padding(x_batch, max(batch_seq_len))\n",
    "print('Origin source chars data:')\n",
    "# print(np.argmax(x_batch, -1))\n",
    "origin = [index2ch[i] for i in np.argmax(x_batch, -1)[0]]\n",
    "print(''.join(map(str, origin)))\n",
    "softmax = tf.nn.softmax(logit, axis=-1)\n",
    "result = tf.argmax(softmax, axis=-1)\n",
    "gen = sess.run(result, feed_dict={X: x_batch, seq_length: batch_seq_len})\n",
    "print('Generated data:')\n",
    "gen = [index2ch[i] for i in gen[0]]\n",
    "print(''.join(map(str, gen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0ehzkZSzv0M"
   },
   "source": [
    "Генерация текста\n",
    "\n",
    "Генерация \"хорошего\" текста требует более детальной настройки сети и более длительного обучения, рекострукция кода происходит практически идеально, что видно выше\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "GPp80PDmzvB-",
    "outputId": "436e2a12-1810-40fd-96ad-4cd6aab03d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated array from noize:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[110,  66,   7,  43,   9,   9,   9,  33,   9, 111,  24,  21,  82,\n",
       "         82,  34, 111,   9,   1,  17,  24,  17,  52,  61,  25, 111,  86,\n",
       "         34,  44,  52,  33,  37,  75,  51,  44,  17,  10,  54,   7,   7,\n",
       "        108,   9,  17,  10,  66,   9, 103,  37,  54,  10,  10,  10,  26,\n",
       "         66,  10, 105, 106,   9,  54,  66,  24,  70, 101,  41,  17,  17,\n",
       "          9,  66,  17,  33,   9, 101,  17,  28,   9,  75,  51,  25,  19,\n",
       "         24,  10,  44, 108,  10,  19,  10, 102,  19,   9,  43,  19,  25,\n",
       "         17,  28,  17,  19,   9,   9,  10,   9,  19]])"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Improvement to test: check encoding, don't use ord and use char directly to make one hot encoding\n",
    "mu, sigma = 0.0, 1.0\n",
    "GEN_SEQ_LEN = 100\n",
    "input = np.random.normal(mu, sigma, (1, GEN_SEQ_LEN, N_LATENT))\n",
    "# source = [chr(ch) for ch in gen[0]]\n",
    "softmax = tf.nn.softmax(logit, axis=-1)\n",
    "result = tf.argmax(softmax, axis=-1)\n",
    "gen = sess.run(result, feed_dict={z_input: input, seq_length: [GEN_SEQ_LEN]})\n",
    "print('Generated array from noize:')\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNSQu5LEPWnK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Autoencoder.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
